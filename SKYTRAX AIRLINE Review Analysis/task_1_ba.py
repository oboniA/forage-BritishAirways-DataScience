# -*- coding: utf-8 -*-
"""Task 1-BA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WVzpt7KOjRiCn2wE6mGW5oUJ-AVSh67v

# Task 1

---

## Web scraping and analysis

This Jupyter notebook includes some code to get you started with web scraping. We will use a package called `BeautifulSoup` to collect the data from the web. Once you've collected your data and saved it into a local `.csv` file you should start with your analysis.

### Scraping data from Skytrax

If you visit [https://www.airlinequality.com] you can see that there is a lot of data there. For this task, we are only interested in reviews related to British Airways and the Airline itself.

If you navigate to this link: [https://www.airlinequality.com/airline-reviews/british-airways] you will see this data. Now, we can use `Python` and `BeautifulSoup` to collect all the links to the reviews and then to collect the text data on each of the individual review links.
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd

base_url = "https://www.airlinequality.com/airline-reviews/british-airways"
pages = 10
page_size = 100

reviews = []

# for i in range(1, pages + 1):
for i in range(1, pages + 1):

    print(f"Scraping page {i}")

    # Create URL to collect links from paginated data
    url = f"{base_url}/page/{i}/?sortby=post_date%3ADesc&pagesize={page_size}"

    # Collect HTML data from this page
    response = requests.get(url)

    # Parse content
    content = response.content
    parsed_content = BeautifulSoup(content, 'html.parser')
    for para in parsed_content.find_all("div", {"class": "text_content"}):
        reviews.append(para.get_text())

    print(f"   ---> {len(reviews)} total reviews")

df = pd.DataFrame()
df["reviews"] = reviews
df.head(10)

df.to_csv("data/BA_reviews.csv")

"""Congratulations! Now you have your dataset for this task! The loops above collected 1000 reviews by iterating through the paginated pages on the website. However, if you want to collect more data, try increasing the number of pages!

 The next thing that you should do is clean this data to remove any unnecessary text from each of the rows. For example, "âœ… Trip Verified" can be removed from each row if it exists, as it's not relevant to what we want to investigate.
"""

import pandas as pd

# Load the dataset
df = pd.read_csv('data/BA_reviews.csv')

# first few rows
df.head()

"""Data Cleaning"""

import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Download stopwords
nltk.download('stopwords')
nltk.download('punkt')

# text cleaning
def text_clean(text):

    # remove special characters and numbers
    text = re.sub(r'\d+', '', text)
    text = re.sub(f"[{re.escape(string.punctuation)}]", "", text)

    # convert to lowercase
    text = text.lower()

    # tokenize text
    words = word_tokenize(text)

    # remove stopwords
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word not in stop_words]

    return ' '.join(words)

# spply cleaning function to reviews column
df['reviews_cleaned'] = df['reviews'].apply(text_clean)
df[['reviews', 'reviews_cleaned']].head(10)

"""Counting word frequencies"""

from collections import Counter

# count word frequencies in reviews
word_counts = Counter(' '.join(df['reviews_cleaned']).split())

# counter to a DataFrame for visualization
word_freq_df = pd.DataFrame(word_counts.items(), columns=['Word', 'Frequency'])

# Sort by frequency in desc order
word_freq_df = word_freq_df.sort_values(by='Frequency', ascending=False)

print(word_freq_df.head(10))

# save the word frequencies to a text file
word_freq_df.to_csv('data/word_frequencies.txt', sep='\t', index=False)

"""Sentiment Analysis"""

from nltk.sentiment import SentimentIntensityAnalyzer
import matplotlib.pyplot as plt
import seaborn as sns

nltk.download('vader_lexicon')

# initialize analyzer
sentiment_analyzer = SentimentIntensityAnalyzer()

# sentiment analysis for compound score
df['sentiment'] = df['reviews_cleaned'].apply(lambda x: sentiment_analyzer.polarity_scores(x)['compound'])

# classify reviews into positive, negative, or neutral
df['sentiment_category'] = df['sentiment'].apply(
    lambda x: 'positive' if x > 0
          else ('negative' if x < 0
          else 'neutral'))

# count number of reviews in each category
sentiment_counts = df['sentiment_category'].value_counts()

# get percentages
total_reviews = sentiment_counts.sum()
percentages = (sentiment_counts / total_reviews) * 100

# plot barchart
plt.figure(figsize=(8, 4))  # Adjust the figure size for a skinnier appearance
barplot = sns.barplot(y=sentiment_counts.index, x=sentiment_counts.values, palette='Set2')

# % values on the bars
for index, value in enumerate(sentiment_counts):
    percentage = f"{percentages[index]:.1f}%"  # Format to 1 decimal place
    barplot.text(value, index, percentage, va='center')  # Add text to the bars

plt.title('Sentiment Distribution', fontsize=16)
plt.ylabel('Sentiment Category', fontsize=14)
plt.xlabel('Number of Reviews', fontsize=14)
plt.show()

print(df['sentiment_category'].unique())

"""WordClouds"""

from wordcloud import WordCloud
import matplotlib.pyplot as plt

"""Wordcloud for positive words"""

# filter positive reviews
pos_reviews = df[df['sentiment_category'] == 'positive']['reviews_cleaned']

# all positive reviews into one large text
pos_text = ' '.join(pos_reviews)
wordcloud_pos = WordCloud(width=800, height=400, background_color='white').generate(pos_text)

# Plot the WordCloud for positive reviews
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_pos, interpolation='bilinear')
plt.axis('off')
plt.title('Common Words in Positive Reviews')
plt.show()

"""Wordcloud for negative words"""

# filter negative reviews
neg_reviews = df[df['sentiment_category'] == 'negative']['reviews_cleaned']

# all negitive reviews into one large text
neg_text = ' '.join(neg_reviews)
wordcloud_neg = WordCloud(width=800, height=400, background_color='white').generate(neg_text)

# Plot the WordCloud for negative reviews
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_neg, interpolation='bilinear')
plt.axis('off')
plt.title('Common Words in Negative Reviews')
plt.show()

# Filter neutral reviews
neu_reviews = df[df['sentiment_category'] == 'neutral']['reviews_cleaned']

# all neutral reviews into one large text
neu_text = ' '.join(neu_reviews)
wordcloud_neu = WordCloud(width=800, height=400, background_color='white').generate(neu_text)

# Plot the WordCloud for neutral reviews
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_pos, interpolation='bilinear')
plt.axis('off')
plt.title('Common Words in Neutral Reviews')
plt.show()

"""word count in entire text

WordCloud to visualize the most overall frequent words in the reviews
"""

# combine all the cleaned reviews into one large text
text = ' '.join(df['reviews_cleaned'])

# generate the word cloud
wc = WordCloud(width=1000, height=500, background_color='white').generate(text)

# plot word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wc, interpolation='bilinear')
plt.axis('off')
plt.show()

"""Topic Modeling using Latent Dirichlet Allocation (LDA)
probabilistic interpretation of topics to understand the contributions of various words to each topic.
"""

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

# vectorize text
vect = CountVectorizer(max_df=0.9, min_df=2, stop_words='english')
vect_fit = vect.fit_transform(df['reviews_cleaned'])

# fit LDA model
lda = LatentDirichletAllocation(n_components=10, random_state=42)
lda.fit(vect_fit)

# repare data for table
topics = []
keywords = []

# display the top words for each topic
for i, topic in enumerate(lda.components_):
    print(f'Topic {i+1}:')
    print([vect.get_feature_names_out()[index] for index in topic.argsort()[-10:]])

    topics.append(f'Topic {i+1}')
    keywords.append([vect.get_feature_names_out()[index] for index in topic.argsort()[-10:]])

# in a table-like format
print(f"{'Topic':<10} | {'Top Keywords'}")
print('-' * 50)
for topic, kw in zip(topics, keywords):
    print(f"{topic:<10} | {', '.join(kw)}")

# topic distribution for each document
topic_distribution = lda.transform(vect_fit)

# sum to find the most discussed topics
topic_sum = topic_distribution.sum(axis=0)

# df to display topic contributions
topic_contributions = pd.DataFrame(topic_sum, index=[f'Topic {i+1}' for i in range(10)], columns=['Topic Contribution'])
topic_contributions = topic_contributions.sort_values(by='Topic Contribution', ascending=False)
print(topic_contributions)

# plot the Topic Distribution Bar Chart
plt.figure(figsize=(10, 6))
topic_contributions.plot(kind='bar', legend=False, color='skyblue')
plt.title('Most Discussed Topics from LDA')
plt.ylabel('Total Contribution')
plt.xlabel('TOPICS')
plt.xticks(rotation=45)
plt.show()

